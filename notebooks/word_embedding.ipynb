{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "064be72f",
   "metadata": {},
   "source": [
    "# Word Embedding Model\n",
    "\n",
    "The architecture used for sentiment analysis is \"Word Embeddings\" whose guide can be viewed at the following link:\n",
    ">https://www.tensorflow.org/text/guide/word_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0867415a",
   "metadata": {},
   "source": [
    "## Creating Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26f5a660",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tweet():\n",
    "    def __init__(self, text, label):\n",
    "        self.text = text\n",
    "        self.label = label\n",
    "\n",
    "class Utils():\n",
    "    def __init__(self, tweets):\n",
    "        self.tweets = tweets\n",
    "        \n",
    "    def get_text(self):\n",
    "        return [x.text for x in self.tweets]\n",
    "    \n",
    "    def get_label(self):\n",
    "        return [x.label for x in self.tweets]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea89e896",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f9f62fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import json\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48376c8c",
   "metadata": {},
   "source": [
    "## Process Data\n",
    "\n",
    "### Read data from json.file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42a8e1f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   awww thats a bummer  you shoulda got david carr of third day to do it d\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "file_name = '../data/Data_processed.json'\n",
    "\n",
    "tweets = []\n",
    "with open(file_name) as f:\n",
    "    for line in f:\n",
    "        tweet = json.loads(line)\n",
    "        tweets.append(Tweet(tweet['Text'], tweet['Target']))\n",
    "    \n",
    "# Taking a look at an example of our data\n",
    "print(tweets[0].text)\n",
    "print(tweets[0].label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25305a84",
   "metadata": {},
   "source": [
    "## Creating our Tensorflow model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4ad8d6",
   "metadata": {},
   "source": [
    "### Setting Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1dd2920a",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1024\n",
    "SEED = 123\n",
    "DENSE_NODES = 16\n",
    "OPTIMIZER = 'adam'\n",
    "METRICS = ['accuracy']\n",
    "EPOCHS = 5\n",
    "VOCAB_SIZE = 10000\n",
    "SEQUENCE_LEN = 50\n",
    "EMBEDDING_DIM = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc1294e",
   "metadata": {},
   "source": [
    "### Creating text/label datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "701bd590",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_text = Utils(tweets).get_text()\n",
    "dataset_labels = Utils(tweets).get_label()\n",
    "\n",
    "ds_labels = tf.convert_to_tensor(dataset_labels)\n",
    "ds_text = tf.convert_to_tensor(dataset_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092c57ed",
   "metadata": {},
   "source": [
    "#### For creating our TextVectorizer Vocab(Encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44ba8bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_text = tf.data.Dataset.from_tensors(ds_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b41311",
   "metadata": {},
   "source": [
    "## Text Vectorization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d44563",
   "metadata": {},
   "source": [
    "Use the text vectorization layer to normalize, split, and map strings to integers. Note that the layer uses the custom standardization defined above.Set maximum_sequence length as all samples are not of the same length."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed05906",
   "metadata": {},
   "source": [
    "Calling adapt mathod to build vocabulary from training dataset while also transforming our test dataset for future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "657f3335",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-17 19:23:24.365125: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 316989920 exceeds 10% of free system memory.\n",
      "2022-02-17 19:23:24.365207: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 475484880 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "vectorize_layer = TextVectorization(standardize='lower_and_strip_punctuation',\n",
    "                                   max_tokens=VOCAB_SIZE,\n",
    "                                   split='whitespace',\n",
    "                                   output_mode='int',\n",
    "                                   output_sequence_length=SEQUENCE_LEN)\n",
    "\n",
    "vectorize_layer.adapt(p_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f363f600",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0e0e377f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    vectorize_layer,\n",
    "    Embedding(VOCAB_SIZE, EMBEDDING_DIM, name='embedding'),\n",
    "    GlobalAveragePooling1D(),\n",
    "    Dense(DENSE_NODES, activation='relu'),\n",
    "    Dense(1, activation='sigmoid') # We want either 0 or 1 for our sentiment analysis\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0b5e13",
   "metadata": {},
   "source": [
    "## Compile and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "497e2a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1407/1407 [==============================] - 28s 20ms/step - loss: 0.5175 - accuracy: 0.7495 - val_loss: 0.5545 - val_accuracy: 0.7429\n",
      "Epoch 2/5\n",
      "1407/1407 [==============================] - 28s 20ms/step - loss: 0.4445 - accuracy: 0.8005 - val_loss: 0.4892 - val_accuracy: 0.7650\n",
      "Epoch 3/5\n",
      "1407/1407 [==============================] - 27s 19ms/step - loss: 0.4318 - accuracy: 0.8033 - val_loss: 0.5232 - val_accuracy: 0.7346\n",
      "Epoch 4/5\n",
      "1407/1407 [==============================] - 27s 19ms/step - loss: 0.4248 - accuracy: 0.8053 - val_loss: 0.5073 - val_accuracy: 0.7451\n",
      "Epoch 5/5\n",
      "1407/1407 [==============================] - 27s 19ms/step - loss: 0.4200 - accuracy: 0.8071 - val_loss: 0.5183 - val_accuracy: 0.7431\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbcd02d4d90>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir='logs') # Saving statistics for tensorboard\n",
    "\n",
    "model.compile(optimizer=OPTIMIZER,\n",
    "             loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "             metrics=METRICS)\n",
    "\n",
    "model.fit(x=ds_text,\n",
    "         y=ds_labels,\n",
    "         batch_size=BATCH_SIZE,\n",
    "         epochs=EPOCHS, \n",
    "         validation_split=0.1,\n",
    "         callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def2dee5",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e68f1a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(x):\n",
    "    if x >= 0.5:\n",
    "        return \"Positive\"\n",
    "    else:\n",
    "        \"Negative\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "07923019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment for this tweet is: Positive\n"
     ]
    }
   ],
   "source": [
    "print(\"Sentiment for this tweet is:\", get_sentiment(model.predict([\"Got praised by Hamza, Yay\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f17ba9",
   "metadata": {},
   "source": [
    "## Visualize model on tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ceef539",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ERROR: Could not find `tensorboard`. Please ensure that your PATH\n",
       "contains an executable `tensorboard` program, or explicitly specify\n",
       "the path to a TensorBoard binary by setting the `TENSORBOARD_BINARY`\n",
       "environment variable."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sa_env",
   "language": "python",
   "name": "sa_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
