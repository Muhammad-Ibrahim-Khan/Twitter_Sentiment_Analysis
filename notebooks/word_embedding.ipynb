{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "064be72f",
   "metadata": {},
   "source": [
    "# Word Embedding Model\n",
    "\n",
    "The architecture used for sentiment analysis is \"Word Embeddings\" whose guide can be viewed at the following link:\n",
    ">https://www.tensorflow.org/text/guide/word_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0867415a",
   "metadata": {},
   "source": [
    "## Creating Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26f5a660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Sentiment():\n",
    "#     Positive = 'Positive'\n",
    "#     Negative = 'Negative'\n",
    "\n",
    "class Tweet():\n",
    "    def __init__(self, text, label):\n",
    "        self.text = text\n",
    "        self.label = label\n",
    "#         self.sentiment = self.get_sentiment()\n",
    "        \n",
    "#     def get_sentiment(self):\n",
    "#         if self.label == 0:\n",
    "#             return Sentiment.Negative\n",
    "#         else:\n",
    "#             return Sentiment.Positive\n",
    "        \n",
    "\n",
    "class Utils():\n",
    "    def __init__(self, tweets):\n",
    "        self.tweets = tweets\n",
    "        \n",
    "    def get_text(self):\n",
    "        return [x.text for x in self.tweets]\n",
    "    \n",
    "#     def get_sentiment(self):\n",
    "#         return [x.sentiment for x in self.tweets]\n",
    "    \n",
    "    def get_label(self):\n",
    "        return [x.label for x in self.tweets]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea89e896",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9f62fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-17 17:40:44.699112: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-02-17 17:40:44.699152: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48376c8c",
   "metadata": {},
   "source": [
    "## Process Data\n",
    "\n",
    "### Read data from json.file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42a8e1f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   awww thats a bummer  you shoulda got david carr of third day to do it d\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "file_name = '../data/Data_processed.json'\n",
    "\n",
    "tweets = []\n",
    "with open(file_name) as f:\n",
    "    for line in f:\n",
    "        tweet = json.loads(line)\n",
    "        tweets.append(Tweet(tweet['Text'], tweet['Target']))\n",
    "    \n",
    "# Taking a look at an example of our data\n",
    "print(tweets[0].text)\n",
    "print(tweets[0].label)\n",
    "# print(tweets[0].sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25305a84",
   "metadata": {},
   "source": [
    "## Creating our Tensorflow model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4ad8d6",
   "metadata": {},
   "source": [
    "### Setting Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1dd2920a",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1024\n",
    "SEED = 123\n",
    "DENSE_NODES = 16\n",
    "OPTIMIZER = 'adam'\n",
    "METRICS = ['accuracy']\n",
    "EPOCHS = 5\n",
    "VOCAB_SIZE = 10000\n",
    "SEQUENCE_LEN = 50\n",
    "EMBEDDING_DIM = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc1294e",
   "metadata": {},
   "source": [
    "## Creating test/train splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98a982c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-17 17:40:53.775940: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-02-17 17:40:53.775985: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-02-17 17:40:53.776004: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (mik-HP-EliteBook-840-G2): /proc/driver/nvidia/version does not exist\n",
      "2022-02-17 17:40:53.776272: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "dataset_text = Utils(tweets).get_text()\n",
    "dataset_labels = Utils(tweets).get_label()\n",
    "\n",
    "ds_labels = tf.convert_to_tensor(dataset_labels)\n",
    "ds_text = tf.convert_to_tensor(dataset_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb13c93",
   "metadata": {},
   "source": [
    "For creating our TextVectorizer Vocab(Embedding vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4f20db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_text = tf.data.Dataset.from_tensors(ds_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f23748d",
   "metadata": {},
   "source": [
    "## Text Vectorization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe77e96",
   "metadata": {},
   "source": [
    "Use the text vectorization layer to normalize, split, and map strings to integers. Note that the layer uses the custom standardization defined above.Set maximum_sequence length as all samples are not of the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d62fa718",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-17 17:40:56.779380: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 316989920 exceeds 10% of free system memory.\n",
      "2022-02-17 17:40:56.779452: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 475484880 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "vectorize_layer = TextVectorization(standardize='lower_and_strip_punctuation',\n",
    "                                   max_tokens=VOCAB_SIZE,\n",
    "                                   split='whitespace',\n",
    "                                   output_mode='int',\n",
    "                                   output_sequence_length=SEQUENCE_LEN)\n",
    "\n",
    "vectorize_layer.adapt(p_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf4fe53",
   "metadata": {},
   "source": [
    "Calling adapt mathod to build vocabulary from training dataset while also transforming our test dataset for future."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b28442c",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7d777fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    vectorize_layer,\n",
    "    Embedding(VOCAB_SIZE, EMBEDDING_DIM, name='embedding'),\n",
    "    GlobalAveragePooling1D(),\n",
    "    Dense(DENSE_NODES, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fa5c76",
   "metadata": {},
   "source": [
    "## Compile and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9cad47c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1407/1407 [==============================] - 28s 19ms/step - loss: 0.5190 - accuracy: 0.7468 - val_loss: 0.5374 - val_accuracy: 0.7648\n",
      "Epoch 2/5\n",
      "1407/1407 [==============================] - 26s 18ms/step - loss: 0.4463 - accuracy: 0.7995 - val_loss: 0.5276 - val_accuracy: 0.7463\n",
      "Epoch 3/5\n",
      "1407/1407 [==============================] - 26s 18ms/step - loss: 0.4330 - accuracy: 0.8020 - val_loss: 0.5203 - val_accuracy: 0.7394\n",
      "Epoch 4/5\n",
      "1407/1407 [==============================] - 25s 18ms/step - loss: 0.4251 - accuracy: 0.8046 - val_loss: 0.5370 - val_accuracy: 0.7245\n",
      "Epoch 5/5\n",
      "1407/1407 [==============================] - 26s 18ms/step - loss: 0.4201 - accuracy: 0.8065 - val_loss: 0.5424 - val_accuracy: 0.7259\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3e9030a160>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir='logs')\n",
    "\n",
    "tf.keras.losses.BinaryCrossentropy\n",
    "\n",
    "model.compile(optimizer=OPTIMIZER,\n",
    "             loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "             metrics=METRICS)\n",
    "\n",
    "model.fit(x=ds_text,\n",
    "         y=ds_labels,\n",
    "         batch_size=BATCH_SIZE,\n",
    "         epochs=EPOCHS, \n",
    "         validation_split=0.1,\n",
    "         callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ece64b0",
   "metadata": {},
   "source": [
    "## Visualize model on tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00cc70c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ERROR: Could not find `tensorboard`. Please ensure that your PATH\n",
       "contains an executable `tensorboard` program, or explicitly specify\n",
       "the path to a TensorBoard binary by setting the `TENSORBOARD_BINARY`\n",
       "environment variable."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sa_env",
   "language": "python",
   "name": "sa_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
